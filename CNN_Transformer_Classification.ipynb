{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f1912c8",
   "metadata": {},
   "source": [
    "# üöÄ Hybrid CNN-Transformer Sleep Disorder Classification\n",
    "\n",
    "## üéØ Architecture Overview:\n",
    "This notebook implements a **hybrid CNN-Transformer model** that combines:\n",
    "- **CNN layers** for local temporal feature extraction\n",
    "- **Transformer encoders** for capturing global dependencies\n",
    "- **Multi-head attention** for learning multiple representation subspaces\n",
    "\n",
    "## üìå Key Advantages:\n",
    "‚úÖ **Global context** - Transformers see the entire sequence at once  \n",
    "‚úÖ **Parallel processing** - Faster than sequential LSTM  \n",
    "‚úÖ **Local patterns** - CNN extracts features efficiently  \n",
    "‚úÖ **Positional encoding** - Preserves temporal information  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59639b7",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Mount Google Drive (For Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3500a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"‚úÖ Google Drive mounted successfully!\")\n",
    "print(\"\\nYour files are now accessible at: /content/drive/MyDrive/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e173ea31",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Install & Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcc7e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages if needed\n",
    "# !pip install -q tensorflow scikit-learn matplotlib seaborn pandas numpy\n",
    "\n",
    "print(\"‚úÖ All packages ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ef29cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, auc, confusion_matrix, classification_report\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, Conv1D, MaxPooling1D, Flatten, Dropout,\n",
    "    BatchNormalization, Activation, GlobalAveragePooling1D, Layer\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from time import time\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"‚úÖ Imports successful!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67bd753",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Data Augmentation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730e1508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jitter(x, sigma=0.03):\n",
    "    \"\"\"Add random Gaussian noise\"\"\"\n",
    "    noise = np.random.normal(loc=0., scale=sigma * np.std(x), size=x.shape)\n",
    "    return x + noise\n",
    "\n",
    "def scaling(x, sigma=0.1):\n",
    "    \"\"\"Randomly scale the signal amplitude\"\"\"\n",
    "    factor = np.random.normal(loc=1., scale=sigma)\n",
    "    return x * factor\n",
    "\n",
    "def time_warp(x, sigma=0.2, knot=4):\n",
    "    \"\"\"Apply time warping to the signal\"\"\"\n",
    "    orig_steps = np.arange(x.shape[0])\n",
    "    random_warps = np.random.normal(loc=1.0, scale=sigma, size=(knot+2,))\n",
    "    warp_steps = (np.linspace(0, x.shape[0]-1, num=knot+2))\n",
    "    ret = np.interp(orig_steps, warp_steps, random_warps)\n",
    "    ret = ret / ret.sum() * x.shape[0]\n",
    "    ret = np.cumsum(ret)\n",
    "    if len(x.shape) == 1:\n",
    "        return np.interp(orig_steps, ret, x)\n",
    "    else:\n",
    "        return np.array([np.interp(orig_steps, ret, x[:, i]) for i in range(x.shape[1])]).T\n",
    "\n",
    "def augment_signal(x, augmentation_list=['jitter', 'scaling', 'time_warp'], n_augmentations=2):\n",
    "    \"\"\"Apply random augmentations to a signal\"\"\"\n",
    "    augmented = x.copy()\n",
    "    selected = np.random.choice(augmentation_list, size=min(n_augmentations, len(augmentation_list)), replace=False)\n",
    "    for aug in selected:\n",
    "        if aug == 'jitter':\n",
    "            augmented = jitter(augmented)\n",
    "        elif aug == 'scaling':\n",
    "            augmented = scaling(augmented)\n",
    "        elif aug == 'time_warp':\n",
    "            augmented = time_warp(augmented)\n",
    "    return augmented\n",
    "\n",
    "def augment_dataset(X, y, augmentation_factor=1):\n",
    "    \"\"\"Augment entire dataset\"\"\"\n",
    "    X_aug_list = [X]\n",
    "    y_aug_list = [y]\n",
    "    for i in range(augmentation_factor):\n",
    "        X_new = np.array([augment_signal(x) for x in X])\n",
    "        X_aug_list.append(X_new)\n",
    "        y_aug_list.append(y)\n",
    "    X_aug = np.concatenate(X_aug_list, axis=0)\n",
    "    y_aug = np.concatenate(y_aug_list, axis=0)\n",
    "    indices = np.random.permutation(len(X_aug))\n",
    "    X_aug = X_aug[indices]\n",
    "    y_aug = y_aug[indices]\n",
    "    print(f\"Original dataset size: {len(X)}\")\n",
    "    print(f\"Augmented dataset size: {len(X_aug)}\")\n",
    "    return X_aug, y_aug\n",
    "\n",
    "print(\"‚úÖ Data augmentation functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3f33ef",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Hybrid CNN-Transformer Architecture üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b27403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding Layer\n",
    "class PositionalEncoding(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(PositionalEncoding, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        seq_len = input_shape[1]\n",
    "        d_model = input_shape[2]\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        position = np.arange(seq_len)[:, np.newaxis]\n",
    "        div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "        \n",
    "        pe = np.zeros((seq_len, d_model))\n",
    "        pe[:, 0::2] = np.sin(position * div_term)\n",
    "        if d_model % 2 == 0:\n",
    "            pe[:, 1::2] = np.cos(position * div_term)\n",
    "        else:\n",
    "            pe[:, 1::2] = np.cos(position * div_term[:-1])\n",
    "        \n",
    "        self.pe = tf.constant(pe, dtype=tf.float32)\n",
    "        super(PositionalEncoding, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        return x + self.pe\n",
    "    \n",
    "    def get_config(self):\n",
    "        return super(PositionalEncoding, self).get_config()\n",
    "\n",
    "\n",
    "# Transformer Encoder Block\n",
    "def transformer_encoder_block(x, num_heads=4, ff_dim=128, dropout_rate=0.1, name_prefix='transformer'):\n",
    "    \"\"\"\n",
    "    Single Transformer Encoder Block with Multi-Head Attention\n",
    "    \"\"\"\n",
    "    # Multi-Head Attention\n",
    "    attn_output = layers.MultiHeadAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=x.shape[-1] // num_heads,\n",
    "        dropout=dropout_rate,\n",
    "        name=f'{name_prefix}_mha'\n",
    "    )(x, x)\n",
    "    attn_output = Dropout(dropout_rate, name=f'{name_prefix}_dropout1')(attn_output)\n",
    "    out1 = layers.LayerNormalization(epsilon=1e-6, name=f'{name_prefix}_ln1')(x + attn_output)\n",
    "    \n",
    "    # Feed-Forward Network\n",
    "    ffn_output = Dense(ff_dim, activation='relu', name=f'{name_prefix}_ffn1')(out1)\n",
    "    ffn_output = Dropout(dropout_rate, name=f'{name_prefix}_dropout2')(ffn_output)\n",
    "    ffn_output = Dense(x.shape[-1], name=f'{name_prefix}_ffn2')(ffn_output)\n",
    "    ffn_output = Dropout(dropout_rate, name=f'{name_prefix}_dropout3')(ffn_output)\n",
    "    out2 = layers.LayerNormalization(epsilon=1e-6, name=f'{name_prefix}_ln2')(out1 + ffn_output)\n",
    "    \n",
    "    return out2\n",
    "\n",
    "\n",
    "# Build Hybrid CNN-Transformer Model\n",
    "def build_cnn_transformer(input_shape, num_transformer_blocks=2, num_heads=4, ff_dim=128):\n",
    "    \"\"\"\n",
    "    Hybrid CNN-Transformer Architecture:\n",
    "    - CNN layers extract local temporal features\n",
    "    - Transformer blocks capture global dependencies\n",
    "    - Best of both worlds!\n",
    "    \"\"\"\n",
    "    input_signal = Input(shape=input_shape, name='input')\n",
    "    \n",
    "    # ============ CNN Feature Extraction ============\n",
    "    # Block 1: Initial convolutions\n",
    "    x = Conv1D(filters=64, kernel_size=7, strides=1, padding='same', name='cnn_conv1')(input_signal)\n",
    "    x = BatchNormalization(name='cnn_bn1')(x)\n",
    "    x = Activation('relu', name='cnn_relu1')(x)\n",
    "    x = MaxPooling1D(pool_size=2, padding='same', name='cnn_pool1')(x)\n",
    "    x = Dropout(0.2, name='cnn_dropout1')(x)\n",
    "    \n",
    "    # Block 2: Deeper features\n",
    "    x = Conv1D(filters=128, kernel_size=5, strides=1, padding='same', name='cnn_conv2')(x)\n",
    "    x = BatchNormalization(name='cnn_bn2')(x)\n",
    "    x = Activation('relu', name='cnn_relu2')(x)\n",
    "    x = MaxPooling1D(pool_size=2, padding='same', name='cnn_pool2')(x)\n",
    "    x = Dropout(0.2, name='cnn_dropout2')(x)\n",
    "    \n",
    "    # Block 3: Feature refinement\n",
    "    x = Conv1D(filters=128, kernel_size=3, strides=1, padding='same', name='cnn_conv3')(x)\n",
    "    x = BatchNormalization(name='cnn_bn3')(x)\n",
    "    x = Activation('relu', name='cnn_relu3')(x)\n",
    "    \n",
    "    # ============ Positional Encoding ============\n",
    "    x = PositionalEncoding(name='pos_encoding')(x)\n",
    "    \n",
    "    # ============ Transformer Encoder Blocks ============\n",
    "    for i in range(num_transformer_blocks):\n",
    "        x = transformer_encoder_block(\n",
    "            x,\n",
    "            num_heads=num_heads,\n",
    "            ff_dim=ff_dim,\n",
    "            dropout_rate=0.1,\n",
    "            name_prefix=f'transformer_block_{i+1}'\n",
    "        )\n",
    "    \n",
    "    # ============ Global Pooling ============\n",
    "    x = GlobalAveragePooling1D(name='global_avg_pool')(x)\n",
    "    \n",
    "    # ============ Classification Head ============\n",
    "    x = Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(0.01), name='dense1')(x)\n",
    "    x = Dropout(0.4, name='dropout_final1')(x)\n",
    "    x = Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(0.01), name='dense2')(x)\n",
    "    x = Dropout(0.3, name='dropout_final2')(x)\n",
    "    output = Dense(1, activation='sigmoid', name='output')(x)\n",
    "    \n",
    "    model = keras.Model(inputs=input_signal, outputs=output, name='CNN_Transformer_Hybrid')\n",
    "    return model\n",
    "\n",
    "print(\"‚úÖ CNN-Transformer hybrid architecture defined!\")\n",
    "print(\"   üîπ CNN layers: Extract local temporal patterns\")\n",
    "print(\"   üîπ Positional encoding: Preserve sequence order\")\n",
    "print(\"   üîπ Transformer blocks: Capture global dependencies\")\n",
    "print(\"   üîπ Multi-head attention: Learn multiple representation subspaces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6c7ad3",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd91a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, title='Training History'):\n",
    "    \"\"\"Plot training and validation metrics\"\"\"\n",
    "    if hasattr(history, 'history'):\n",
    "        history = history.history\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[0].plot(history['accuracy'], 'b-', linewidth=2, label='Training')\n",
    "    axes[0].plot(history['val_accuracy'], 'r-', linewidth=2, label='Validation')\n",
    "    axes[0].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Loss\n",
    "    axes[1].plot(history['loss'], 'b-', linewidth=2, label='Training')\n",
    "    axes[1].plot(history['val_loss'], 'r-', linewidth=2, label='Validation')\n",
    "    axes[1].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names=['Healthy', 'Unhealthy']):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    sns.heatmap(cm_norm, annot=True, fmt='.2%', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                linewidths=2, linecolor='white', ax=ax)\n",
    "    \n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j + 0.5, i + 0.7, f'({cm[i, j]})',\n",
    "                   ha='center', va='center', fontsize=10, color='gray')\n",
    "    \n",
    "    ax.set_ylabel('True Label', fontsize=13, fontweight='bold')\n",
    "    ax.set_xlabel('Predicted Label', fontsize=13, fontweight='bold')\n",
    "    ax.set_title('Confusion Matrix', fontsize=15, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc_curve(y_true, y_pred_proba):\n",
    "    \"\"\"Plot ROC curve\"\"\"\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=3, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
    "    plt.fill_between(fpr, tpr, 0, alpha=0.2, color='orange')\n",
    "    plt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "    plt.title('ROC Curve', fontsize=13, fontweight='bold')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    return roc_auc\n",
    "\n",
    "def generate_metrics_report(y_true, y_pred, y_pred_proba):\n",
    "    \"\"\"Generate comprehensive metrics\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    \n",
    "    if cm.shape == (2, 2):\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    else:\n",
    "        specificity = 0\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä PERFORMANCE METRICS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Accuracy:     {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"Precision:    {precision:.4f} ({precision*100:.2f}%)\")\n",
    "    print(f\"Recall:       {recall:.4f} ({recall*100:.2f}%)\")\n",
    "    print(f\"Specificity:  {specificity:.4f} ({specificity*100:.2f}%)\")\n",
    "    print(f\"F1-Score:     {f1:.4f} ({f1*100:.2f}%)\")\n",
    "    print(f\"ROC-AUC:      {roc_auc:.4f} ({roc_auc*100:.2f}%)\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    print(\"CLASSIFICATION REPORT:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=['Healthy', 'Unhealthy'], digits=4))\n",
    "    \n",
    "    return {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'Specificity': specificity,\n",
    "        'F1-Score': f1,\n",
    "        'ROC-AUC': roc_auc\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Visualization functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b630a47",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Load Dataset\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT: Update DATA_PATH below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13187225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update this path to your dataset location\n",
    "DATA_PATH = '/content/drive/MyDrive/your_folder/healthy_unhealthy1.csv'\n",
    "\n",
    "# Load data\n",
    "data = np.loadtxt(DATA_PATH, delimiter=',')\n",
    "print(f\"‚úÖ Data loaded successfully!\")\n",
    "print(f\"   Shape: {data.shape}\")\n",
    "\n",
    "# Split features and labels\n",
    "X = data[:, 0:1024]  # First 1024 columns\n",
    "y = data[:, -1]      # Last column (label)\n",
    "\n",
    "print(f\"\\nüìä Dataset Statistics:\")\n",
    "print(f\"   Total samples: {len(X)}\")\n",
    "print(f\"   Feature dimensions: {X.shape[1]}\")\n",
    "print(f\"   Healthy samples: {np.sum(y == 0)} ({np.sum(y == 0)/len(y)*100:.1f}%)\")\n",
    "print(f\"   Unhealthy samples: {np.sum(y == 1)} ({np.sum(y == 1)/len(y)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e58133",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Data Preparation & Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e949b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, shuffle=True, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"üìä Data Split:\")\n",
    "print(f\"   Training: {len(X_train)} samples\")\n",
    "print(f\"   Test: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfd25e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply augmentation\n",
    "APPLY_AUGMENTATION = True\n",
    "AUGMENTATION_FACTOR = 1  # Increase for more augmented data\n",
    "\n",
    "if APPLY_AUGMENTATION and AUGMENTATION_FACTOR > 0:\n",
    "    print(\"üîÑ Applying data augmentation...\")\n",
    "    X_train_aug, y_train_aug = augment_dataset(X_train, y_train, AUGMENTATION_FACTOR)\n",
    "else:\n",
    "    X_train_aug, y_train_aug = X_train, y_train\n",
    "\n",
    "# Reshape for CNN-Transformer input\n",
    "X_train_aug = X_train_aug.reshape(-1, 1024, 1)\n",
    "X_test_reshaped = X_test.reshape(-1, 1024, 1)\n",
    "\n",
    "print(f\"\\n‚úÖ Final shapes:\")\n",
    "print(f\"   Training: {X_train_aug.shape}\")\n",
    "print(f\"   Test: {X_test_reshaped.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b592adc6",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Build & Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25428e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build CNN-Transformer model\n",
    "input_shape = (1024, 1)\n",
    "\n",
    "print(\"üèóÔ∏è Building CNN-Transformer model...\\n\")\n",
    "model = build_cnn_transformer(\n",
    "    input_shape,\n",
    "    num_transformer_blocks=2,  # Number of transformer encoder blocks\n",
    "    num_heads=4,                # Number of attention heads\n",
    "    ff_dim=128                  # Feed-forward dimension\n",
    ")\n",
    "\n",
    "# Compile\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "print(f\"\\n‚úÖ Model built successfully!\")\n",
    "print(f\"   Total parameters: {model.count_params():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b2cb86",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad12225a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "EPOCHS = 150\n",
    "BATCH_SIZE = 64\n",
    "MODEL_SAVE_PATH = '/content/drive/MyDrive/cnn_transformer_model.h5'\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=30,\n",
    "        verbose=1,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=10,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        MODEL_SAVE_PATH,\n",
    "        monitor='val_accuracy',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='max'\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"üöÄ Starting training...\\n\")\n",
    "start_time = time()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_aug, y_train_aug,\n",
    "    validation_split=0.2,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "training_time = time() - start_time\n",
    "\n",
    "print(f\"\\n‚úÖ Training completed!\")\n",
    "print(f\"   Total time: {training_time:.2f}s ({training_time/60:.2f} min)\")\n",
    "print(f\"   Model saved to: {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e9c334",
   "metadata": {},
   "source": [
    "## üîü Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf4fc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "print(\"üìä Generating predictions...\\n\")\n",
    "y_pred_proba = model.predict(X_test_reshaped, verbose=0).flatten()\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(history, 'CNN-Transformer Training')\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot ROC curve\n",
    "plot_roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "# Generate metrics report\n",
    "metrics = generate_metrics_report(y_test, y_pred, y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bfea8e",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffc5adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéä CNN-TRANSFORMER MODEL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä Dataset:\")\n",
    "print(f\"   Total samples: {len(X)}\")\n",
    "print(f\"   Training samples: {len(X_train_aug)}\")\n",
    "print(f\"   Test samples: {len(X_test)}\")\n",
    "\n",
    "print(\"\\nüèÜ Model Performance:\")\n",
    "print(f\"   Accuracy: {metrics['Accuracy']*100:.2f}%\")\n",
    "print(f\"   F1-Score: {metrics['F1-Score']*100:.2f}%\")\n",
    "print(f\"   ROC-AUC: {metrics['ROC-AUC']*100:.2f}%\")\n",
    "print(f\"   Precision: {metrics['Precision']*100:.2f}%\")\n",
    "print(f\"   Recall: {metrics['Recall']*100:.2f}%\")\n",
    "\n",
    "print(\"\\nüíæ Saved Artifacts:\")\n",
    "print(f\"   Model: {MODEL_SAVE_PATH}\")\n",
    "\n",
    "print(\"\\nüéØ Architecture Features:\")\n",
    "print(\"   ‚úÖ CNN layers for local feature extraction\")\n",
    "print(\"   ‚úÖ Positional encoding for temporal information\")\n",
    "print(\"   ‚úÖ Multi-head attention for global dependencies\")\n",
    "print(\"   ‚úÖ Transformer encoders for sequence modeling\")\n",
    "print(\"   ‚úÖ Layer normalization for training stability\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ CNN-TRANSFORMER CLASSIFICATION COMPLETED!\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87aded4",
   "metadata": {},
   "source": [
    "## üìù Next Steps\n",
    "\n",
    "### Model Improvements:\n",
    "- Adjust `num_transformer_blocks` (2-4 blocks)\n",
    "- Experiment with `num_heads` (4, 8, or 16)\n",
    "- Tune `ff_dim` (feed-forward dimension)\n",
    "- Try different learning rates\n",
    "- Increase augmentation factor\n",
    "\n",
    "### Analysis:\n",
    "- Compare with BiLSTM-CNN-Attention model\n",
    "- Analyze attention weights\n",
    "- Perform cross-validation\n",
    "- Test on external datasets\n",
    "\n",
    "---\n",
    "\n",
    "**üöÄ The hybrid CNN-Transformer combines the best of both worlds for time-series classification!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
