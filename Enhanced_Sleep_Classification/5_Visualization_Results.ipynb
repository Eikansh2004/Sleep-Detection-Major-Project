{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f04224ef",
   "metadata": {},
   "source": [
    "# Visualization & Results Analysis\n",
    "This notebook provides comprehensive visualization tools for model performance analysis.\n",
    "\n",
    "## Visualization Tools Included:\n",
    "1. **Training History Plots**: Loss and accuracy curves\n",
    "2. **Confusion Matrix**: Detailed classification results\n",
    "3. **ROC & PR Curves**: Model discrimination ability\n",
    "4. **Attention Weights**: What the model focuses on\n",
    "5. **Feature Importance**: Signal analysis\n",
    "6. **Prediction Analysis**: Correct vs incorrect predictions\n",
    "7. **Comparative Analysis**: Multiple model comparison\n",
    "8. **Statistical Reports**: Comprehensive metrics tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d15930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report, \n",
    "    roc_curve, auc, precision_recall_curve,\n",
    "    accuracy_score, precision_score, recall_score, f1_score\n",
    ")\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "\n",
    "print(\"‚úÖ Visualization libraries loaded\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec70cd1e",
   "metadata": {},
   "source": [
    "## 1. Training History Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a824c035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot training and validation loss/accuracy\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    history : keras History object or dict\n",
    "        Training history\n",
    "    save_path : str or None\n",
    "        Path to save figure\n",
    "    \"\"\"\n",
    "    if hasattr(history, 'history'):\n",
    "        history = history.history\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    axes[0].plot(history['accuracy'], 'b-', linewidth=2, label='Training Accuracy')\n",
    "    axes[0].plot(history['val_accuracy'], 'r-', linewidth=2, label='Validation Accuracy')\n",
    "    axes[0].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_title('Model Accuracy Over Epochs', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend(loc='lower right', fontsize=11)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add final values\n",
    "    final_train_acc = history['accuracy'][-1]\n",
    "    final_val_acc = history['val_accuracy'][-1]\n",
    "    axes[0].text(0.02, 0.98, f'Final Train: {final_train_acc:.4f}\\nFinal Val: {final_val_acc:.4f}',\n",
    "                transform=axes[0].transAxes, fontsize=10,\n",
    "                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    # Plot loss\n",
    "    axes[1].plot(history['loss'], 'b-', linewidth=2, label='Training Loss')\n",
    "    axes[1].plot(history['val_loss'], 'r-', linewidth=2, label='Validation Loss')\n",
    "    axes[1].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_title('Model Loss Over Epochs', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend(loc='upper right', fontsize=11)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add final values\n",
    "    final_train_loss = history['loss'][-1]\n",
    "    final_val_loss = history['val_loss'][-1]\n",
    "    axes[1].text(0.02, 0.98, f'Final Train: {final_train_loss:.4f}\\nFinal Val: {final_val_loss:.4f}',\n",
    "                transform=axes[1].transAxes, fontsize=10,\n",
    "                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight')\n",
    "        print(f\"‚úÖ Training history saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "print(\"‚úÖ Training history plotting function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c8d00c",
   "metadata": {},
   "source": [
    "## 2. Enhanced Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faebd115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, class_names=['Healthy', 'Unhealthy'], \n",
    "                         normalize=False, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot enhanced confusion matrix with percentages\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array\n",
    "        True labels\n",
    "    y_pred : array\n",
    "        Predicted labels\n",
    "    class_names : list\n",
    "        Names of classes\n",
    "    normalize : bool\n",
    "        Whether to show percentages\n",
    "    save_path : str or None\n",
    "        Path to save figure\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    if normalize:\n",
    "        cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        cm_display = cm_norm\n",
    "        fmt = '.2%'\n",
    "    else:\n",
    "        cm_display = cm\n",
    "        fmt = 'd'\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(cm_display, annot=True, fmt=fmt, cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                cbar_kws={'label': 'Count' if not normalize else 'Percentage'},\n",
    "                linewidths=2, linecolor='white', ax=ax)\n",
    "    \n",
    "    # Add counts in each cell\n",
    "    if normalize:\n",
    "        for i in range(cm.shape[0]):\n",
    "            for j in range(cm.shape[1]):\n",
    "                ax.text(j + 0.5, i + 0.7, f'({cm[i, j]})',\n",
    "                       ha='center', va='center', fontsize=10, color='gray')\n",
    "    \n",
    "    ax.set_ylabel('True Label', fontsize=13, fontweight='bold')\n",
    "    ax.set_xlabel('Predicted Label', fontsize=13, fontweight='bold')\n",
    "    ax.set_title('Confusion Matrix', fontsize=15, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    if cm.shape == (2, 2):\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "        \n",
    "        metrics_text = f'Sensitivity: {sensitivity:.3f}\\nSpecificity: {specificity:.3f}\\n'\n",
    "        metrics_text += f'Precision: {precision:.3f}\\nNPV: {npv:.3f}'\n",
    "        \n",
    "        ax.text(1.15, 0.5, metrics_text, transform=ax.transAxes,\n",
    "               fontsize=11, verticalalignment='center',\n",
    "               bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight')\n",
    "        print(f\"‚úÖ Confusion matrix saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "print(\"‚úÖ Confusion matrix plotting function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d90058",
   "metadata": {},
   "source": [
    "## 3. ROC & Precision-Recall Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c39076f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_and_pr_curves(y_true, y_pred_proba, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot ROC curve and Precision-Recall curve side by side\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array\n",
    "        True labels\n",
    "    y_pred_proba : array\n",
    "        Predicted probabilities\n",
    "    save_path : str or None\n",
    "        Path to save figure\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # ROC Curve\n",
    "    fpr, tpr, thresholds_roc = roc_curve(y_true, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    axes[0].plot(fpr, tpr, color='darkorange', lw=3, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "    axes[0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "    axes[0].fill_between(fpr, tpr, 0, alpha=0.2, color='orange')\n",
    "    axes[0].set_xlim([0.0, 1.0])\n",
    "    axes[0].set_ylim([0.0, 1.05])\n",
    "    axes[0].set_xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_title('Receiver Operating Characteristic (ROC) Curve', fontsize=13, fontweight='bold')\n",
    "    axes[0].legend(loc='lower right', fontsize=11)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Precision-Recall Curve\n",
    "    precision, recall, thresholds_pr = precision_recall_curve(y_true, y_pred_proba)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    \n",
    "    axes[1].plot(recall, precision, color='green', lw=3, label=f'PR curve (AUC = {pr_auc:.3f})')\n",
    "    axes[1].fill_between(recall, precision, 0, alpha=0.2, color='green')\n",
    "    axes[1].set_xlim([0.0, 1.0])\n",
    "    axes[1].set_ylim([0.0, 1.05])\n",
    "    axes[1].set_xlabel('Recall', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_ylabel('Precision', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_title('Precision-Recall Curve', fontsize=13, fontweight='bold')\n",
    "    axes[1].legend(loc='lower left', fontsize=11)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight')\n",
    "        print(f\"‚úÖ ROC and PR curves saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return roc_auc, pr_auc\n",
    "\n",
    "print(\"‚úÖ ROC and PR curve plotting function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fdf659",
   "metadata": {},
   "source": [
    "## 4. Attention Weights Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c3ac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_weights(model, X_sample, sample_idx=0, save_path=None):\n",
    "    \"\"\"\n",
    "    Visualize attention weights for a specific sample\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : keras.Model\n",
    "        Model with attention layer\n",
    "    X_sample : array\n",
    "        Input samples\n",
    "    sample_idx : int\n",
    "        Index of sample to visualize\n",
    "    save_path : str or None\n",
    "        Path to save figure\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get attention layer\n",
    "        attention_layer = None\n",
    "        for layer in model.layers:\n",
    "            if 'attention' in layer.name.lower():\n",
    "                attention_layer = layer\n",
    "                break\n",
    "        \n",
    "        if attention_layer is None:\n",
    "            print(\"‚ö†Ô∏è No attention layer found in model\")\n",
    "            return\n",
    "        \n",
    "        # Create model to output attention weights\n",
    "        attention_model = keras.Model(\n",
    "            inputs=model.input,\n",
    "            outputs=attention_layer.output\n",
    "        )\n",
    "        \n",
    "        # Get attention weights\n",
    "        sample = X_sample[sample_idx:sample_idx+1]\n",
    "        _, attention_weights = attention_model.predict(sample, verbose=0)\n",
    "        attention_weights = attention_weights[0].flatten()\n",
    "        \n",
    "        # Plot\n",
    "        fig, axes = plt.subplots(2, 1, figsize=(15, 8))\n",
    "        \n",
    "        # Original signal\n",
    "        axes[0].plot(sample[0].flatten(), 'b-', linewidth=1.5, alpha=0.7)\n",
    "        axes[0].set_title('Original Signal', fontsize=13, fontweight='bold')\n",
    "        axes[0].set_ylabel('Amplitude', fontsize=11)\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Attention weights\n",
    "        axes[1].plot(attention_weights, 'r-', linewidth=1.5)\n",
    "        axes[1].fill_between(range(len(attention_weights)), attention_weights, alpha=0.3, color='red')\n",
    "        axes[1].set_title('Attention Weights (What the model focuses on)', fontsize=13, fontweight='bold')\n",
    "        axes[1].set_xlabel('Time Steps', fontsize=11)\n",
    "        axes[1].set_ylabel('Attention Weight', fontsize=11)\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, bbox_inches='tight')\n",
    "            print(f\"‚úÖ Attention visualization saved to {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not visualize attention: {e}\")\n",
    "\n",
    "print(\"‚úÖ Attention visualization function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353a93a4",
   "metadata": {},
   "source": [
    "## 5. Prediction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296ce1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction_analysis(X_test, y_test, y_pred, y_pred_proba, \n",
    "                            class_names=['Healthy', 'Unhealthy'],\n",
    "                            num_samples=6, save_path=None):\n",
    "    \"\"\"\n",
    "    Visualize correct and incorrect predictions\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_test : array\n",
    "        Test samples\n",
    "    y_test : array\n",
    "        True labels\n",
    "    y_pred : array\n",
    "        Predicted labels\n",
    "    y_pred_proba : array\n",
    "        Prediction probabilities\n",
    "    class_names : list\n",
    "        Class names\n",
    "    num_samples : int\n",
    "        Number of samples to show\n",
    "    save_path : str or None\n",
    "        Path to save figure\n",
    "    \"\"\"\n",
    "    # Find correct and incorrect predictions\n",
    "    correct_idx = np.where(y_test == y_pred)[0]\n",
    "    incorrect_idx = np.where(y_test != y_pred)[0]\n",
    "    \n",
    "    # Select samples\n",
    "    n_correct = min(num_samples // 2, len(correct_idx))\n",
    "    n_incorrect = min(num_samples // 2, len(incorrect_idx))\n",
    "    \n",
    "    selected_correct = np.random.choice(correct_idx, n_correct, replace=False)\n",
    "    selected_incorrect = np.random.choice(incorrect_idx, n_incorrect, replace=False) if len(incorrect_idx) > 0 else []\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 1, figsize=(14, 3*num_samples))\n",
    "    \n",
    "    plot_idx = 0\n",
    "    \n",
    "    # Plot correct predictions\n",
    "    for idx in selected_correct:\n",
    "        if plot_idx >= num_samples:\n",
    "            break\n",
    "        ax = axes[plot_idx] if num_samples > 1 else axes\n",
    "        ax.plot(X_test[idx].flatten(), 'g-', linewidth=1, alpha=0.8)\n",
    "        ax.set_title(f'‚úÖ CORRECT: True={class_names[int(y_test[idx])]}, '\n",
    "                    f'Pred={class_names[int(y_pred[idx])]} (Confidence: {y_pred_proba[idx]:.3f})',\n",
    "                    fontsize=11, fontweight='bold', color='green')\n",
    "        ax.set_ylabel('Amplitude')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plot_idx += 1\n",
    "    \n",
    "    # Plot incorrect predictions\n",
    "    for idx in selected_incorrect:\n",
    "        if plot_idx >= num_samples:\n",
    "            break\n",
    "        ax = axes[plot_idx] if num_samples > 1 else axes\n",
    "        ax.plot(X_test[idx].flatten(), 'r-', linewidth=1, alpha=0.8)\n",
    "        ax.set_title(f'‚ùå INCORRECT: True={class_names[int(y_test[idx])]}, '\n",
    "                    f'Pred={class_names[int(y_pred[idx])]} (Confidence: {y_pred_proba[idx]:.3f})',\n",
    "                    fontsize=11, fontweight='bold', color='red')\n",
    "        ax.set_ylabel('Amplitude')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plot_idx += 1\n",
    "    \n",
    "    if num_samples > 1:\n",
    "        axes[-1].set_xlabel('Time Steps', fontsize=11)\n",
    "    else:\n",
    "        axes.set_xlabel('Time Steps', fontsize=11)\n",
    "    \n",
    "    plt.suptitle('Prediction Analysis: Correct vs Incorrect Classifications', \n",
    "                fontsize=14, fontweight='bold', y=1.00)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight')\n",
    "        print(f\"‚úÖ Prediction analysis saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "print(\"‚úÖ Prediction analysis function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03e6789",
   "metadata": {},
   "source": [
    "## 6. Comprehensive Metrics Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e7fc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_metrics_report(y_true, y_pred, y_pred_proba, class_names=['Healthy', 'Unhealthy']):\n",
    "    \"\"\"\n",
    "    Generate comprehensive metrics report\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array\n",
    "        True labels\n",
    "    y_pred : array\n",
    "        Predicted labels\n",
    "    y_pred_proba : array\n",
    "        Prediction probabilities\n",
    "    class_names : list\n",
    "        Class names\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    metrics_dict : dict\n",
    "        Dictionary of all metrics\n",
    "    \"\"\"\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Calculate basic metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    \n",
    "    # ROC-AUC\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    except:\n",
    "        roc_auc = 0.0\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    if cm.shape == (2, 2):\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "        fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    else:\n",
    "        specificity = npv = fpr = fnr = 0\n",
    "    \n",
    "    metrics_dict = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision (PPV)': precision,\n",
    "        'Recall (Sensitivity/TPR)': recall,\n",
    "        'Specificity (TNR)': specificity,\n",
    "        'F1-Score': f1,\n",
    "        'ROC-AUC': roc_auc,\n",
    "        'NPV': npv,\n",
    "        'FPR': fpr,\n",
    "        'FNR': fnr\n",
    "    }\n",
    "    \n",
    "    # Print report\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä COMPREHENSIVE METRICS REPORT\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nTotal Samples: {len(y_true)}\")\n",
    "    print(f\"Class Distribution: {np.bincount(y_true.astype(int))}\")\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"PERFORMANCE METRICS:\")\n",
    "    print(\"-\"*70)\n",
    "    for metric, value in metrics_dict.items():\n",
    "        print(f\"  {metric:30s}: {value:.4f} ({value*100:.2f}%)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CLASSIFICATION REPORT:\")\n",
    "    print(\"=\"*70)\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CONFUSION MATRIX:\")\n",
    "    print(\"=\"*70)\n",
    "    print(pd.DataFrame(cm, index=class_names, columns=class_names))\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    return metrics_dict\n",
    "\n",
    "print(\"‚úÖ Metrics report function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818c3271",
   "metadata": {},
   "source": [
    "## 7. Model Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245e3f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comparison_table(results_dict, save_path=None):\n",
    "    \"\"\"\n",
    "    Create comparison table for multiple models\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results_dict : dict\n",
    "        Dictionary with model names as keys and metrics dicts as values\n",
    "    save_path : str or None\n",
    "        Path to save table as CSV\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    df : DataFrame\n",
    "        Comparison table\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(results_dict).T\n",
    "    df = df.round(4)\n",
    "    \n",
    "    # Style the dataframe\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"üìä MODEL COMPARISON TABLE\")\n",
    "    print(\"=\"*100)\n",
    "    print(df.to_string())\n",
    "    print(\"=\"*100 + \"\\n\")\n",
    "    \n",
    "    # Highlight best values\n",
    "    print(\"\\nüèÜ BEST PERFORMING MODEL PER METRIC:\")\n",
    "    print(\"-\"*100)\n",
    "    for col in df.columns:\n",
    "        best_model = df[col].idxmax()\n",
    "        best_value = df[col].max()\n",
    "        print(f\"  {col:30s}: {best_model:20s} ({best_value:.4f})\")\n",
    "    print(\"=\"*100 + \"\\n\")\n",
    "    \n",
    "    if save_path:\n",
    "        df.to_csv(save_path)\n",
    "        print(f\"‚úÖ Comparison table saved to {save_path}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"‚úÖ Comparison table function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f66d5f4",
   "metadata": {},
   "source": [
    "## 8. Complete Visualization Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c01bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_visualization_pipeline(model, history, X_test, y_test, \n",
    "                                   class_names=['Healthy', 'Unhealthy'],\n",
    "                                   save_dir='./results'):\n",
    "    \"\"\"\n",
    "    Run complete visualization pipeline\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : keras.Model\n",
    "        Trained model\n",
    "    history : keras History or dict\n",
    "        Training history\n",
    "    X_test, y_test : arrays\n",
    "        Test data\n",
    "    class_names : list\n",
    "        Class names\n",
    "    save_dir : str\n",
    "        Directory to save all plots\n",
    "    \"\"\"\n",
    "    import os\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üé® Running Complete Visualization Pipeline\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # Get predictions\n",
    "    print(\"üìä Generating predictions...\")\n",
    "    y_pred_proba = model.predict(X_test, verbose=0).flatten()\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    # 1. Training history\n",
    "    print(\"üìà Plotting training history...\")\n",
    "    plot_training_history(history, save_path=f\"{save_dir}/training_history.png\")\n",
    "    \n",
    "    # 2. Confusion matrix\n",
    "    print(\"üìä Creating confusion matrix...\")\n",
    "    plot_confusion_matrix(y_test, y_pred, class_names, \n",
    "                         normalize=True, save_path=f\"{save_dir}/confusion_matrix.png\")\n",
    "    \n",
    "    # 3. ROC and PR curves\n",
    "    print(\"üìà Plotting ROC and PR curves...\")\n",
    "    plot_roc_and_pr_curves(y_test, y_pred_proba, save_path=f\"{save_dir}/roc_pr_curves.png\")\n",
    "    \n",
    "    # 4. Prediction analysis\n",
    "    print(\"üîç Analyzing predictions...\")\n",
    "    plot_prediction_analysis(X_test, y_test, y_pred, y_pred_proba, class_names,\n",
    "                            num_samples=6, save_path=f\"{save_dir}/prediction_analysis.png\")\n",
    "    \n",
    "    # 5. Metrics report\n",
    "    print(\"üìä Generating metrics report...\")\n",
    "    metrics = generate_metrics_report(y_test, y_pred, y_pred_proba, class_names)\n",
    "    \n",
    "    # 6. Attention visualization (if applicable)\n",
    "    print(\"üëÅÔ∏è Attempting attention visualization...\")\n",
    "    visualize_attention_weights(model, X_test, sample_idx=0, \n",
    "                               save_path=f\"{save_dir}/attention_weights.png\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"‚úÖ All visualizations saved to {save_dir}/\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"‚úÖ Complete visualization pipeline defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc903a3e",
   "metadata": {},
   "source": [
    "## Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63edd30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (uncomment to use):\n",
    "# \n",
    "# # After training your model\n",
    "# # model = ...\n",
    "# # history = ...\n",
    "# # X_test, y_test = ...\n",
    "# \n",
    "# # Run complete visualization\n",
    "# metrics = complete_visualization_pipeline(\n",
    "#     model=model,\n",
    "#     history=history,\n",
    "#     X_test=X_test,\n",
    "#     y_test=y_test,\n",
    "#     class_names=['Healthy', 'Unhealthy'],\n",
    "#     save_dir='./my_results'\n",
    "# )\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ Visualization utilities loaded successfully!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nAvailable functions:\")\n",
    "print(\"  - plot_training_history(history, save_path)\")\n",
    "print(\"  - plot_confusion_matrix(y_true, y_pred, class_names, normalize, save_path)\")\n",
    "print(\"  - plot_roc_and_pr_curves(y_true, y_pred_proba, save_path)\")\n",
    "print(\"  - visualize_attention_weights(model, X_sample, sample_idx, save_path)\")\n",
    "print(\"  - plot_prediction_analysis(X_test, y_test, y_pred, y_pred_proba, ...)\")\n",
    "print(\"  - generate_metrics_report(y_true, y_pred, y_pred_proba, class_names)\")\n",
    "print(\"  - create_comparison_table(results_dict, save_path)\")\n",
    "print(\"  - complete_visualization_pipeline(model, history, X_test, y_test, ...)\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
