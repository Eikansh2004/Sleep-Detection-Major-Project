{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51cd1451",
   "metadata": {},
   "source": [
    "# Attention-Enhanced BiLSTM-CNN Model\n",
    "This notebook implements an advanced hybrid architecture combining:\n",
    "- **Bidirectional LSTM** for temporal context\n",
    "- **1D CNN** for local feature extraction\n",
    "- **Attention Mechanism** for important feature focus\n",
    "- **Skip Connections** for gradient flow\n",
    "\n",
    "## Model Architecture Benefits:\n",
    "- ‚úÖ Captures both past and future context (BiLSTM)\n",
    "- ‚úÖ Learns hierarchical features (CNN)\n",
    "- ‚úÖ Focuses on discriminative patterns (Attention)\n",
    "- ‚úÖ Better gradient propagation (Skip connections)\n",
    "- ‚úÖ Reduces overfitting (Dropout, BatchNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9389ba1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.layers as tfl\n",
    "from tensorflow.keras.layers import *\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1209d54c",
   "metadata": {},
   "source": [
    "## 1. Attention Layer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616c5a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(Layer):\n",
    "    \"\"\"\n",
    "    Custom Attention Layer for time series\n",
    "    \n",
    "    This layer computes attention weights for each time step,\n",
    "    allowing the model to focus on the most relevant parts of the signal.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='attention_weight',\n",
    "                                 shape=(input_shape[-1], input_shape[-1]),\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(name='attention_bias',\n",
    "                                 shape=(input_shape[-1],),\n",
    "                                 initializer='zeros',\n",
    "                                 trainable=True)\n",
    "        self.u = self.add_weight(name='attention_context',\n",
    "                                 shape=(input_shape[-1],),\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        # Compute attention scores\n",
    "        uit = tf.tanh(tf.tensordot(x, self.W, axes=1) + self.b)\n",
    "        ait = tf.tensordot(uit, self.u, axes=1)\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = tf.nn.softmax(ait, axis=1)\n",
    "        attention_weights = tf.expand_dims(attention_weights, axis=-1)\n",
    "        \n",
    "        # Apply attention weights\n",
    "        weighted_input = x * attention_weights\n",
    "        \n",
    "        return tf.reduce_sum(weighted_input, axis=1), attention_weights\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "    \n",
    "    def get_config(self):\n",
    "        return super(AttentionLayer, self).get_config()\n",
    "\n",
    "print(\"‚úÖ Attention layer defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ee25d8",
   "metadata": {},
   "source": [
    "## 2. Attention-Enhanced BiLSTM-CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a703f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_attention_bilstm_cnn(input_shape, use_attention=True):\n",
    "    \"\"\"\n",
    "    Build Attention-Enhanced BiLSTM-CNN model for sleep disorder classification\n",
    "    \n",
    "    Architecture:\n",
    "    1. Initial CNN block for local feature extraction\n",
    "    2. Skip connection block with batch normalization\n",
    "    3. Bidirectional LSTM for temporal dependencies\n",
    "    4. Attention mechanism to focus on important features\n",
    "    5. Dense layers for classification\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_shape : tuple\n",
    "        Shape of input (timesteps, features)\n",
    "    use_attention : bool\n",
    "        Whether to use attention mechanism\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    model : keras.Model\n",
    "        Compiled model\n",
    "    \"\"\"\n",
    "    \n",
    "    input_signal = Input(shape=input_shape, name='input')\n",
    "    \n",
    "    # ====== Block 1: Initial Feature Extraction ======\n",
    "    x = Conv1D(filters=32, kernel_size=7, strides=1, padding='same', name='conv1_1')(input_signal)\n",
    "    x = BatchNormalization(name='bn1_1')(x)\n",
    "    x = Activation('relu', name='relu1_1')(x)\n",
    "    \n",
    "    x = Conv1D(filters=32, kernel_size=7, strides=1, padding='same', name='conv1_2')(x)\n",
    "    x = BatchNormalization(name='bn1_2')(x)\n",
    "    x = Activation('relu', name='relu1_2')(x)\n",
    "    \n",
    "    # Skip connection 1\n",
    "    skip1 = x\n",
    "    \n",
    "    # ====== Block 2: Deeper Feature Extraction with Skip ======\n",
    "    x = Conv1D(filters=32, kernel_size=9, strides=1, padding='same', name='conv2_1')(x)\n",
    "    x = BatchNormalization(name='bn2_1')(x)\n",
    "    x = Activation('relu', name='relu2_1')(x)\n",
    "    \n",
    "    x = Conv1D(filters=32, kernel_size=9, strides=1, padding='same', name='conv2_2')(x)\n",
    "    x = BatchNormalization(name='bn2_2')(x)\n",
    "    \n",
    "    # Add skip connection\n",
    "    x = Add(name='skip_add_1')([x, skip1])\n",
    "    x = Activation('relu', name='relu2_3')(x)\n",
    "    \n",
    "    # ====== Block 3: Downsampling ======\n",
    "    x = Conv1D(filters=64, kernel_size=9, strides=1, padding='same', name='conv3_1')(x)\n",
    "    x = BatchNormalization(name='bn3_1')(x)\n",
    "    x = Activation('relu', name='relu3_1')(x)\n",
    "    x = MaxPooling1D(pool_size=2, padding='same', name='pool1')(x)\n",
    "    x = Dropout(0.3, name='dropout1')(x)\n",
    "    \n",
    "    # ====== Block 4: More CNN layers ======\n",
    "    x = Conv1D(filters=64, kernel_size=7, strides=1, padding='same', name='conv4_1')(x)\n",
    "    x = BatchNormalization(name='bn4_1')(x)\n",
    "    x = Activation('relu', name='relu4_1')(x)\n",
    "    \n",
    "    x = Conv1D(filters=32, kernel_size=5, strides=1, padding='same', name='conv4_2')(x)\n",
    "    x = BatchNormalization(name='bn4_2')(x)\n",
    "    x = Activation('relu', name='relu4_2')(x)\n",
    "    x = MaxPooling1D(pool_size=2, padding='same', name='pool2')(x)\n",
    "    \n",
    "    # ====== Block 5: Bidirectional LSTM ======\n",
    "    x = Bidirectional(LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), \n",
    "                      name='bilstm1')(x)\n",
    "    x = BatchNormalization(name='bn_lstm1')(x)\n",
    "    \n",
    "    x = Bidirectional(LSTM(32, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), \n",
    "                      name='bilstm2')(x)\n",
    "    x = BatchNormalization(name='bn_lstm2')(x)\n",
    "    \n",
    "    # ====== Block 6: Attention Mechanism ======\n",
    "    if use_attention:\n",
    "        x, attention_weights = AttentionLayer(name='attention')(x)\n",
    "    else:\n",
    "        x = GlobalAveragePooling1D(name='global_avg_pool')(x)\n",
    "    \n",
    "    # ====== Block 7: Dense Classification Layers ======\n",
    "    x = Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(0.01), \n",
    "              name='dense1')(x)\n",
    "    x = Dropout(0.4, name='dropout2')(x)\n",
    "    \n",
    "    x = Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(0.01), \n",
    "              name='dense2')(x)\n",
    "    x = Dropout(0.3, name='dropout3')(x)\n",
    "    \n",
    "    # Output layer\n",
    "    output = Dense(1, activation='sigmoid', name='output')(x)\n",
    "    \n",
    "    # Build model\n",
    "    model = keras.Model(inputs=input_signal, outputs=output, name='AttentionBiLSTM_CNN')\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"‚úÖ Model architecture defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5704d217",
   "metadata": {},
   "source": [
    "## 3. Compile Model with Advanced Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332c2276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_attention_model(model, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Compile model with Adam optimizer and binary crossentropy\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : keras.Model\n",
    "        Model to compile\n",
    "    learning_rate : float\n",
    "        Initial learning rate\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    model : keras.Model\n",
    "        Compiled model\n",
    "    \"\"\"\n",
    "    optimizer = keras.optimizers.Adam(\n",
    "        learning_rate=learning_rate,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-07\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', \n",
    "                 tf.keras.metrics.Precision(name='precision'),\n",
    "                 tf.keras.metrics.Recall(name='recall'),\n",
    "                 tf.keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"‚úÖ Compilation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cbeae5",
   "metadata": {},
   "source": [
    "## 4. Training Function with Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f474e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_attention_model(model, x_train, y_train, x_val=None, y_val=None, \n",
    "                         epochs=150, batch_size=64, save_path=None):\n",
    "    \"\"\"\n",
    "    Train the attention model with advanced callbacks\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : keras.Model\n",
    "        Model to train\n",
    "    x_train, y_train : arrays\n",
    "        Training data\n",
    "    x_val, y_val : arrays or None\n",
    "        Validation data (if None, will use validation_split)\n",
    "    epochs : int\n",
    "        Maximum number of epochs\n",
    "    batch_size : int\n",
    "        Batch size\n",
    "    save_path : str or None\n",
    "        Path to save best model\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    history : History object\n",
    "        Training history\n",
    "    training_time : float\n",
    "        Total training time in seconds\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define callbacks\n",
    "    callbacks_list = []\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=30,\n",
    "        verbose=1,\n",
    "        restore_best_weights=True,\n",
    "        mode='min'\n",
    "    )\n",
    "    callbacks_list.append(early_stopping)\n",
    "    \n",
    "    # Reduce learning rate on plateau\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=10,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1,\n",
    "        mode='min'\n",
    "    )\n",
    "    callbacks_list.append(reduce_lr)\n",
    "    \n",
    "    # Model checkpoint\n",
    "    if save_path:\n",
    "        checkpoint = ModelCheckpoint(\n",
    "            save_path,\n",
    "            monitor='val_accuracy',\n",
    "            verbose=1,\n",
    "            save_best_only=True,\n",
    "            mode='max'\n",
    "        )\n",
    "        callbacks_list.append(checkpoint)\n",
    "    \n",
    "    # Start training\n",
    "    start_time = time()\n",
    "    \n",
    "    if x_val is not None and y_val is not None:\n",
    "        # Use provided validation data\n",
    "        history = model.fit(\n",
    "            x_train, y_train,\n",
    "            validation_data=(x_val, y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=callbacks_list,\n",
    "            verbose=1,\n",
    "            shuffle=True\n",
    "        )\n",
    "    else:\n",
    "        # Use validation split\n",
    "        history = model.fit(\n",
    "            x_train, y_train,\n",
    "            validation_split=0.2,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=callbacks_list,\n",
    "            verbose=1,\n",
    "            shuffle=True\n",
    "        )\n",
    "    \n",
    "    training_time = time() - start_time\n",
    "    \n",
    "    print(f\"\\n‚úÖ Training completed in {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
    "    print(f\"üìä Final training accuracy: {history.history['accuracy'][-1]*100:.2f}%\")\n",
    "    print(f\"üìä Final validation accuracy: {history.history['val_accuracy'][-1]*100:.2f}%\")\n",
    "    \n",
    "    return history, training_time\n",
    "\n",
    "print(\"‚úÖ Training function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3346addb",
   "metadata": {},
   "source": [
    "## 5. Simple CNN-LSTM Baseline (for comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a916bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_simple_cnn_lstm(input_shape):\n",
    "    \"\"\"\n",
    "    Build a simpler CNN-LSTM baseline model for comparison\n",
    "    \"\"\"\n",
    "    model = keras.Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        Conv1D(32, 7, activation='relu', padding='same'),\n",
    "        MaxPooling1D(2),\n",
    "        Conv1D(64, 5, activation='relu', padding='same'),\n",
    "        MaxPooling1D(2),\n",
    "        LSTM(64, return_sequences=True),\n",
    "        LSTM(32, return_sequences=False),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ], name='Simple_CNN_LSTM')\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"‚úÖ Baseline model defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b01a0a",
   "metadata": {},
   "source": [
    "## 6. Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a2d49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create and compile model\n",
    "# Uncomment to test\n",
    "\n",
    "# input_shape = (1024, 1)  # 1024 time steps, 1 feature\n",
    "# \n",
    "# # Build attention model\n",
    "# model = build_attention_bilstm_cnn(input_shape, use_attention=True)\n",
    "# model = compile_attention_model(model, learning_rate=0.001)\n",
    "# \n",
    "# # Display model summary\n",
    "# model.summary()\n",
    "# \n",
    "# # Count parameters\n",
    "# total_params = model.count_params()\n",
    "# print(f\"\\nüìä Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c12348c",
   "metadata": {},
   "source": [
    "## 7. Model Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6389ebc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model_architecture(model, save_path='model_architecture.png'):\n",
    "    \"\"\"\n",
    "    Visualize and save model architecture\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from tensorflow.keras.utils import plot_model\n",
    "        plot_model(model, to_file=save_path, show_shapes=True, \n",
    "                   show_layer_names=True, rankdir='TB', dpi=96)\n",
    "        print(f\"‚úÖ Model architecture saved to {save_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not visualize model: {e}\")\n",
    "        print(\"Install graphviz and pydot if needed: pip install pydot graphviz\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ Attention-Enhanced BiLSTM-CNN Model loaded successfully!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nAvailable functions:\")\n",
    "print(\"  - build_attention_bilstm_cnn(input_shape, use_attention)\")\n",
    "print(\"  - compile_attention_model(model, learning_rate)\")\n",
    "print(\"  - train_attention_model(model, x_train, y_train, ...)\")\n",
    "print(\"  - build_simple_cnn_lstm(input_shape) [baseline]\")\n",
    "print(\"  - visualize_model_architecture(model, save_path)\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
