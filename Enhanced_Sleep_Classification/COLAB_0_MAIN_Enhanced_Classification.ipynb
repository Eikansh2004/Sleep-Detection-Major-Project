{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f93ae864",
   "metadata": {},
   "source": [
    "# üéØ Enhanced Sleep Disorder Classification - Complete Pipeline (GOOGLE COLAB VERSION)\n",
    "\n",
    "## üìå Setup Instructions for Google Colab:\n",
    "1. **Upload your dataset** to Google Drive\n",
    "2. **Mount Google Drive** (run the cell below)\n",
    "3. **Update DATA_PATH** to point to your CSV file\n",
    "4. **Run all cells** in order\n",
    "\n",
    "## üöÄ Key Improvements:\n",
    "- Attention-Enhanced BiLSTM-CNN\n",
    "- Advanced Data Augmentation  \n",
    "- K-Fold Cross-Validation\n",
    "- Model Ensemble\n",
    "- Comprehensive Visualization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80cbc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"‚úÖ Google Drive mounted successfully!\")\n",
    "print(\"\\nYour files are now accessible at: /content/drive/MyDrive/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1ed5a0",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Install Required Packages (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7185ebaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install any missing packages\n",
    "# !pip install -q tensorflow scikit-learn matplotlib seaborn pandas numpy scipy\n",
    "\n",
    "print(\"‚úÖ All packages ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a3b261",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79705ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    roc_auc_score, roc_curve, auc, precision_recall_curve,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, Conv1D, MaxPooling1D, AveragePooling1D, MaxPool1D,\n",
    "    Flatten, Dropout, BatchNormalization, Activation, Add, Concatenate,\n",
    "    LSTM, Bidirectional, GlobalAveragePooling1D, Layer\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from scipy.interpolate import interp1d\n",
    "from time import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"‚úÖ Imports successful!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb37ba0",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Data Augmentation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4256ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation Utilities\n",
    "\n",
    "def time_warp(x, sigma=0.2, knot=4):\n",
    "    \"\"\"Apply time warping to the signal\"\"\"\n",
    "    orig_steps = np.arange(x.shape[0])\n",
    "    random_warps = np.random.normal(loc=1.0, scale=sigma, size=(knot+2,))\n",
    "    warp_steps = (np.linspace(0, x.shape[0]-1, num=knot+2))\n",
    "    ret = np.interp(orig_steps, warp_steps, random_warps)\n",
    "    ret = ret / ret.sum() * x.shape[0]\n",
    "    ret = np.cumsum(ret)\n",
    "    if len(x.shape) == 1:\n",
    "        return np.interp(orig_steps, ret, x)\n",
    "    else:\n",
    "        return np.array([np.interp(orig_steps, ret, x[:, i]) for i in range(x.shape[1])]).T\n",
    "\n",
    "def magnitude_warp(x, sigma=0.2, knot=4):\n",
    "    \"\"\"Apply magnitude warping to the signal\"\"\"\n",
    "    orig_steps = np.arange(x.shape[0])\n",
    "    random_warps = np.random.normal(loc=1.0, scale=sigma, size=(knot+2,))\n",
    "    warp_steps = (np.linspace(0, x.shape[0]-1, num=knot+2))\n",
    "    warper = np.interp(orig_steps, warp_steps, random_warps)\n",
    "    if len(x.shape) == 1:\n",
    "        return x * warper\n",
    "    else:\n",
    "        return x * warper[:, np.newaxis]\n",
    "\n",
    "def jitter(x, sigma=0.03):\n",
    "    \"\"\"Add random Gaussian noise\"\"\"\n",
    "    noise = np.random.normal(loc=0., scale=sigma * np.std(x), size=x.shape)\n",
    "    return x + noise\n",
    "\n",
    "def scaling(x, sigma=0.1):\n",
    "    \"\"\"Randomly scale the signal amplitude\"\"\"\n",
    "    factor = np.random.normal(loc=1., scale=sigma)\n",
    "    return x * factor\n",
    "\n",
    "def time_shift(x, shift_range=0.1):\n",
    "    \"\"\"Randomly shift the signal in time (circular)\"\"\"\n",
    "    shift = int(np.random.uniform(-shift_range, shift_range) * x.shape[0])\n",
    "    return np.roll(x, shift, axis=0)\n",
    "\n",
    "def window_slice(x, reduce_ratio=0.9):\n",
    "    \"\"\"Randomly crop and resize the signal\"\"\"\n",
    "    target_len = int(reduce_ratio * x.shape[0])\n",
    "    if target_len >= x.shape[0]:\n",
    "        return x\n",
    "    start = np.random.randint(0, x.shape[0] - target_len)\n",
    "    end = start + target_len\n",
    "    sliced = x[start:end]\n",
    "    if len(x.shape) == 1:\n",
    "        return np.interp(np.arange(x.shape[0]), np.arange(len(sliced)), sliced)\n",
    "    else:\n",
    "        return np.array([np.interp(np.arange(x.shape[0]), np.arange(len(sliced)), sliced[:, i]) \n",
    "                        for i in range(x.shape[1])]).T\n",
    "\n",
    "def rotation(x):\n",
    "    \"\"\"Randomly flip/invert the signal\"\"\"\n",
    "    flip = np.random.choice([-1, 1])\n",
    "    return flip * x\n",
    "\n",
    "def augment_signal(x, augmentation_list=['jitter', 'scaling', 'time_warp', 'magnitude_warp'], n_augmentations=2):\n",
    "    \"\"\"Apply random augmentations to a signal\"\"\"\n",
    "    augmented = x.copy()\n",
    "    selected = np.random.choice(augmentation_list, size=n_augmentations, replace=False)\n",
    "    for aug in selected:\n",
    "        if aug == 'jitter':\n",
    "            augmented = jitter(augmented)\n",
    "        elif aug == 'scaling':\n",
    "            augmented = scaling(augmented)\n",
    "        elif aug == 'time_warp':\n",
    "            augmented = time_warp(augmented)\n",
    "        elif aug == 'magnitude_warp':\n",
    "            augmented = magnitude_warp(augmented)\n",
    "        elif aug == 'time_shift':\n",
    "            augmented = time_shift(augmented)\n",
    "        elif aug == 'window_slice':\n",
    "            augmented = window_slice(augmented)\n",
    "        elif aug == 'rotation':\n",
    "            augmented = rotation(augmented)\n",
    "    return augmented\n",
    "\n",
    "def augment_dataset(X, y, augmentation_factor=2, augmentation_methods=['jitter', 'scaling', 'time_warp']):\n",
    "    \"\"\"Augment entire dataset\"\"\"\n",
    "    X_aug_list = [X]\n",
    "    y_aug_list = [y]\n",
    "    for i in range(augmentation_factor):\n",
    "        X_new = np.array([augment_signal(x, augmentation_methods, n_augmentations=2) for x in X])\n",
    "        X_aug_list.append(X_new)\n",
    "        y_aug_list.append(y)\n",
    "    X_aug = np.concatenate(X_aug_list, axis=0)\n",
    "    y_aug = np.concatenate(y_aug_list, axis=0)\n",
    "    indices = np.random.permutation(len(X_aug))\n",
    "    X_aug = X_aug[indices]\n",
    "    y_aug = y_aug[indices]\n",
    "    print(f\"Original dataset size: {len(X)}\")\n",
    "    print(f\"Augmented dataset size: {len(X_aug)}\")\n",
    "    print(f\"Augmentation factor: {len(X_aug) / len(X):.2f}x\")\n",
    "    return X_aug, y_aug\n",
    "\n",
    "print(\"‚úÖ Data augmentation functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6435919d",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Attention Layer & Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54ece67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Attention Layer\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='attention_weight',\n",
    "                                 shape=(input_shape[-1], input_shape[-1]),\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(name='attention_bias',\n",
    "                                 shape=(input_shape[-1],),\n",
    "                                 initializer='zeros',\n",
    "                                 trainable=True)\n",
    "        self.u = self.add_weight(name='attention_context',\n",
    "                                 shape=(input_shape[-1],),\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        uit = tf.tanh(tf.tensordot(x, self.W, axes=1) + self.b)\n",
    "        ait = tf.tensordot(uit, self.u, axes=1)\n",
    "        attention_weights = tf.nn.softmax(ait, axis=1)\n",
    "        attention_weights = tf.expand_dims(attention_weights, axis=-1)\n",
    "        weighted_input = x * attention_weights\n",
    "        return tf.reduce_sum(weighted_input, axis=1), attention_weights\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "    \n",
    "    def get_config(self):\n",
    "        return super(AttentionLayer, self).get_config()\n",
    "\n",
    "# Build Attention-Enhanced BiLSTM-CNN Model\n",
    "def build_attention_bilstm_cnn(input_shape, use_attention=True):\n",
    "    input_signal = Input(shape=input_shape, name='input')\n",
    "    \n",
    "    # Block 1: Initial Feature Extraction\n",
    "    x = Conv1D(filters=32, kernel_size=7, strides=1, padding='same', name='conv1_1')(input_signal)\n",
    "    x = BatchNormalization(name='bn1_1')(x)\n",
    "    x = Activation('relu', name='relu1_1')(x)\n",
    "    x = Conv1D(filters=32, kernel_size=7, strides=1, padding='same', name='conv1_2')(x)\n",
    "    x = BatchNormalization(name='bn1_2')(x)\n",
    "    x = Activation('relu', name='relu1_2')(x)\n",
    "    skip1 = x\n",
    "    \n",
    "    # Block 2: Deeper Feature Extraction with Skip\n",
    "    x = Conv1D(filters=32, kernel_size=9, strides=1, padding='same', name='conv2_1')(x)\n",
    "    x = BatchNormalization(name='bn2_1')(x)\n",
    "    x = Activation('relu', name='relu2_1')(x)\n",
    "    x = Conv1D(filters=32, kernel_size=9, strides=1, padding='same', name='conv2_2')(x)\n",
    "    x = BatchNormalization(name='bn2_2')(x)\n",
    "    x = Add(name='skip_add_1')([x, skip1])\n",
    "    x = Activation('relu', name='relu2_3')(x)\n",
    "    \n",
    "    # Block 3: Downsampling\n",
    "    x = Conv1D(filters=64, kernel_size=9, strides=1, padding='same', name='conv3_1')(x)\n",
    "    x = BatchNormalization(name='bn3_1')(x)\n",
    "    x = Activation('relu', name='relu3_1')(x)\n",
    "    x = MaxPooling1D(pool_size=2, padding='same', name='pool1')(x)\n",
    "    x = Dropout(0.3, name='dropout1')(x)\n",
    "    \n",
    "    # Block 4: More CNN layers\n",
    "    x = Conv1D(filters=64, kernel_size=7, strides=1, padding='same', name='conv4_1')(x)\n",
    "    x = BatchNormalization(name='bn4_1')(x)\n",
    "    x = Activation('relu', name='relu4_1')(x)\n",
    "    x = Conv1D(filters=32, kernel_size=5, strides=1, padding='same', name='conv4_2')(x)\n",
    "    x = BatchNormalization(name='bn4_2')(x)\n",
    "    x = Activation('relu', name='relu4_2')(x)\n",
    "    x = MaxPooling1D(pool_size=2, padding='same', name='pool2')(x)\n",
    "    \n",
    "    # Block 5: Bidirectional LSTM\n",
    "    x = Bidirectional(LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), name='bilstm1')(x)\n",
    "    x = BatchNormalization(name='bn_lstm1')(x)\n",
    "    x = Bidirectional(LSTM(32, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), name='bilstm2')(x)\n",
    "    x = BatchNormalization(name='bn_lstm2')(x)\n",
    "    \n",
    "    # Block 6: Attention Mechanism\n",
    "    if use_attention:\n",
    "        x, attention_weights = AttentionLayer(name='attention')(x)\n",
    "    else:\n",
    "        x = GlobalAveragePooling1D(name='global_avg_pool')(x)\n",
    "    \n",
    "    # Block 7: Dense Classification Layers\n",
    "    x = Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(0.01), name='dense1')(x)\n",
    "    x = Dropout(0.4, name='dropout2')(x)\n",
    "    x = Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(0.01), name='dense2')(x)\n",
    "    x = Dropout(0.3, name='dropout3')(x)\n",
    "    output = Dense(1, activation='sigmoid', name='output')(x)\n",
    "    \n",
    "    model = keras.Model(inputs=input_signal, outputs=output, name='AttentionBiLSTM_CNN')\n",
    "    return model\n",
    "\n",
    "# Baseline CNN-LSTM\n",
    "def build_simple_cnn_lstm(input_shape):\n",
    "    model = keras.Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        Conv1D(32, 7, activation='relu', padding='same'),\n",
    "        MaxPooling1D(2),\n",
    "        Conv1D(64, 5, activation='relu', padding='same'),\n",
    "        MaxPooling1D(2),\n",
    "        LSTM(64, return_sequences=True),\n",
    "        LSTM(32, return_sequences=False),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ], name='Simple_CNN_LSTM')\n",
    "    return model\n",
    "\n",
    "print(\"‚úÖ Model architectures defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e43c08b",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31824722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Utilities\n",
    "\n",
    "def plot_training_history(history, save_path=None):\n",
    "    \"\"\"Plot training and validation loss/accuracy\"\"\"\n",
    "    if hasattr(history, 'history'):\n",
    "        history = history.history\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    axes[0].plot(history['accuracy'], 'b-', linewidth=2, label='Training Accuracy')\n",
    "    axes[0].plot(history['val_accuracy'], 'r-', linewidth=2, label='Validation Accuracy')\n",
    "    axes[0].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_title('Model Accuracy Over Epochs', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend(loc='lower right', fontsize=11)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot loss\n",
    "    axes[1].plot(history['loss'], 'b-', linewidth=2, label='Training Loss')\n",
    "    axes[1].plot(history['val_loss'], 'r-', linewidth=2, label='Validation Loss')\n",
    "    axes[1].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_title('Model Loss Over Epochs', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend(loc='upper right', fontsize=11)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names=['Healthy', 'Unhealthy'], normalize=True, save_path=None):\n",
    "    \"\"\"Plot enhanced confusion matrix\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    if normalize:\n",
    "        cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        cm_display = cm_norm\n",
    "        fmt = '.2%'\n",
    "    else:\n",
    "        cm_display = cm\n",
    "        fmt = 'd'\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    sns.heatmap(cm_display, annot=True, fmt=fmt, cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                linewidths=2, linecolor='white', ax=ax)\n",
    "    \n",
    "    if normalize:\n",
    "        for i in range(cm.shape[0]):\n",
    "            for j in range(cm.shape[1]):\n",
    "                ax.text(j + 0.5, i + 0.7, f'({cm[i, j]})',\n",
    "                       ha='center', va='center', fontsize=10, color='gray')\n",
    "    \n",
    "    ax.set_ylabel('True Label', fontsize=13, fontweight='bold')\n",
    "    ax.set_xlabel('Predicted Label', fontsize=13, fontweight='bold')\n",
    "    ax.set_title('Confusion Matrix', fontsize=15, fontweight='bold', pad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc_curve(y_true, y_pred_proba, save_path=None):\n",
    "    \"\"\"Plot ROC curve\"\"\"\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=3, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "    plt.fill_between(fpr, tpr, 0, alpha=0.2, color='orange')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=13, fontweight='bold')\n",
    "    plt.legend(loc='lower right', fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    return roc_auc\n",
    "\n",
    "def generate_metrics_report(y_true, y_pred, y_pred_proba, class_names=['Healthy', 'Unhealthy']):\n",
    "    \"\"\"Generate comprehensive metrics report\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    except:\n",
    "        roc_auc = 0.0\n",
    "    \n",
    "    if cm.shape == (2, 2):\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    else:\n",
    "        specificity = 0\n",
    "    \n",
    "    metrics_dict = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall (Sensitivity)': recall,\n",
    "        'Specificity': specificity,\n",
    "        'F1-Score': f1,\n",
    "        'ROC-AUC': roc_auc\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä COMPREHENSIVE METRICS REPORT\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nTotal Samples: {len(y_true)}\")\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"PERFORMANCE METRICS:\")\n",
    "    print(\"-\"*70)\n",
    "    for metric, value in metrics_dict.items():\n",
    "        print(f\"  {metric:30s}: {value:.4f} ({value*100:.2f}%)\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nCLASSIFICATION REPORT:\")\n",
    "    print(\"=\"*70)\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n",
    "    print(\"\\nCONFUSION MATRIX:\")\n",
    "    print(\"=\"*70)\n",
    "    print(pd.DataFrame(cm, index=class_names, columns=class_names))\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    return metrics_dict\n",
    "\n",
    "print(\"‚úÖ Visualization functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ee1947",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Load Dataset\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT: Update DATA_PATH below to your dataset location in Google Drive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68649068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update this path to your dataset location in Google Drive\n",
    "# Example: '/content/drive/MyDrive/ML/H&U_classification/dataset/healthy_unhealthy1.csv'\n",
    "DATA_PATH = '/content/drive/MyDrive/your_folder/healthy_unhealthy1.csv'\n",
    "\n",
    "# Load data\n",
    "try:\n",
    "    data = np.loadtxt(DATA_PATH, delimiter=',')\n",
    "    print(f\"‚úÖ Data loaded successfully!\")\n",
    "    print(f\"   Shape: {data.shape}\")\n",
    "    \n",
    "    # Split features and labels\n",
    "    X = data[:, 0:1024]  # First 1024 columns are features\n",
    "    y = data[:, -1]      # Last column is label (0=Healthy, 1=Unhealthy)\n",
    "    \n",
    "    print(f\"\\nüìä Dataset Statistics:\")\n",
    "    print(f\"   Total samples: {len(X)}\")\n",
    "    print(f\"   Feature dimensions: {X.shape[1]}\")\n",
    "    print(f\"   Healthy samples: {np.sum(y == 0)} ({np.sum(y == 0)/len(y)*100:.1f}%)\")\n",
    "    print(f\"   Unhealthy samples: {np.sum(y == 1)} ({np.sum(y == 1)/len(y)*100:.1f}%)\")\n",
    "    print(f\"   Class balance ratio: {np.sum(y == 0) / np.sum(y == 1):.2f}:1\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Data file not found!\")\n",
    "    print(\"Please update DATA_PATH with your dataset location.\")\n",
    "    print(f\"Current path: {DATA_PATH}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d99eb29",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Data Preparation & Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e770802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, shuffle=True, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"üìä Data Split:\")\n",
    "print(f\"   Training set: {len(X_train)} samples\")\n",
    "print(f\"   Test set: {len(X_test)} samples\")\n",
    "print(f\"   Train class distribution: {np.bincount(y_train.astype(int))}\")\n",
    "print(f\"   Test class distribution: {np.bincount(y_test.astype(int))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf27b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply data augmentation\n",
    "APPLY_AUGMENTATION = True  # Set to False to skip augmentation\n",
    "AUGMENTATION_FACTOR = 1     # Increase for more augmented data\n",
    "\n",
    "if APPLY_AUGMENTATION and AUGMENTATION_FACTOR > 0:\n",
    "    print(\"üîÑ Applying data augmentation...\")\n",
    "    X_train_aug, y_train_aug = augment_dataset(\n",
    "        X_train, y_train,\n",
    "        augmentation_factor=AUGMENTATION_FACTOR,\n",
    "        augmentation_methods=['jitter', 'scaling', 'time_warp', 'magnitude_warp']\n",
    "    )\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping data augmentation\")\n",
    "    X_train_aug, y_train_aug = X_train, y_train\n",
    "\n",
    "# Reshape for CNN input\n",
    "X_train_aug = X_train_aug.reshape(-1, 1024, 1)\n",
    "X_test_reshaped = X_test.reshape(-1, 1024, 1)\n",
    "\n",
    "print(f\"\\n‚úÖ Final training data shape: {X_train_aug.shape}\")\n",
    "print(f\"‚úÖ Final test data shape: {X_test_reshaped.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8b58a0",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Build & Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0bc2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build attention model\n",
    "input_shape = (1024, 1)\n",
    "\n",
    "print(\"üèóÔ∏è Building Attention-Enhanced BiLSTM-CNN model...\")\n",
    "model = build_attention_bilstm_cnn(input_shape, use_attention=True)\n",
    "\n",
    "# Compile model\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "print(f\"\\n‚úÖ Model built successfully!\")\n",
    "print(f\"   Total parameters: {model.count_params():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34003dd0",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef2c87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "EPOCHS = 150\n",
    "BATCH_SIZE = 64\n",
    "MODEL_SAVE_PATH = '/content/drive/MyDrive/best_attention_model.h5'\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', patience=30, verbose=1, restore_best_weights=True, mode='min'\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6, verbose=1, mode='min'\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    MODEL_SAVE_PATH, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max'\n",
    ")\n",
    "\n",
    "print(\"üöÄ Starting model training...\\n\")\n",
    "start_time = time()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_aug, y_train_aug,\n",
    "    validation_split=0.2,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stopping, reduce_lr, checkpoint],\n",
    "    verbose=1,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "training_time = time() - start_time\n",
    "\n",
    "print(f\"\\n‚úÖ Training completed!\")\n",
    "print(f\"   Total time: {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
    "print(f\"   Model saved to: {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1983d7df",
   "metadata": {},
   "source": [
    "## üîü Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2560f16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "y_pred_proba = model.predict(X_test_reshaped, verbose=0).flatten()\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Visualizations\n",
    "plot_training_history(history)\n",
    "plot_confusion_matrix(y_test, y_pred, normalize=True)\n",
    "plot_roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "# Comprehensive metrics\n",
    "attention_metrics = generate_metrics_report(y_test, y_pred, y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e2d9f1",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Compare with Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cb5ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline model\n",
    "print(\"üîÑ Training baseline CNN-LSTM model for comparison...\\n\")\n",
    "\n",
    "baseline_model = build_simple_cnn_lstm((1024, 1))\n",
    "baseline_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "baseline_history = baseline_model.fit(\n",
    "    X_train_aug, y_train_aug,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Baseline model trained!\")\n",
    "\n",
    "# Baseline predictions\n",
    "y_pred_baseline_proba = baseline_model.predict(X_test_reshaped, verbose=0).flatten()\n",
    "y_pred_baseline = (y_pred_baseline_proba > 0.5).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "baseline_metrics = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_baseline),\n",
    "    'F1-Score': f1_score(y_test, y_pred_baseline),\n",
    "    'ROC-AUC': roc_auc_score(y_test, y_pred_baseline_proba)\n",
    "}\n",
    "\n",
    "# Create comparison\n",
    "comparison = pd.DataFrame({\n",
    "    'Attention-BiLSTM-CNN': [attention_metrics['Accuracy'], attention_metrics['F1-Score'], attention_metrics['ROC-AUC']],\n",
    "    'Baseline-CNN-LSTM': [baseline_metrics['Accuracy'], baseline_metrics['F1-Score'], baseline_metrics['ROC-AUC']]\n",
    "}, index=['Accuracy', 'F1-Score', 'ROC-AUC'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(comparison)\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate improvements\n",
    "print(\"\\nüìà Improvement Over Baseline:\")\n",
    "for metric in ['Accuracy', 'F1-Score', 'ROC-AUC']:\n",
    "    improvement = (comparison.loc[metric, 'Attention-BiLSTM-CNN'] - \n",
    "                   comparison.loc[metric, 'Baseline-CNN-LSTM']) * 100\n",
    "    print(f\"   {metric}: {improvement:+.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3bdcde",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£2Ô∏è‚É£ Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d8b00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéä PROJECT SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä Dataset:\")\n",
    "print(f\"   Total samples: {len(X)}\")\n",
    "print(f\"   Training samples: {len(X_train_aug)}\")\n",
    "print(f\"   Test samples: {len(X_test)}\")\n",
    "\n",
    "print(\"\\nüèÜ Best Model Performance:\")\n",
    "print(f\"   Accuracy: {attention_metrics['Accuracy']*100:.2f}%\")\n",
    "print(f\"   F1-Score: {attention_metrics['F1-Score']*100:.2f}%\")\n",
    "print(f\"   ROC-AUC: {attention_metrics['ROC-AUC']*100:.2f}%\")\n",
    "\n",
    "print(\"\\nüíæ Saved Artifacts:\")\n",
    "print(f\"   Model saved to: {MODEL_SAVE_PATH}\")\n",
    "\n",
    "print(\"\\nüéØ Key Techniques Used:\")\n",
    "print(\"   ‚úÖ Attention mechanism for feature focus\")\n",
    "print(\"   ‚úÖ Bidirectional LSTM for temporal context\")\n",
    "print(\"   ‚úÖ Skip connections for gradient flow\")\n",
    "print(\"   ‚úÖ Batch normalization for stability\")\n",
    "if APPLY_AUGMENTATION:\n",
    "    print(\"   ‚úÖ Data augmentation for generalization\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ PROJECT COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7abaa7",
   "metadata": {},
   "source": [
    "## üìù Next Steps\n",
    "\n",
    "### For Project Submission:\n",
    "1. **Screenshot all visualizations** (training curves, confusion matrix, ROC curve)\n",
    "2. **Save the comparison table** to Google Drive\n",
    "3. **Download the trained model** from Google Drive\n",
    "4. **Prepare presentation** highlighting improvements\n",
    "\n",
    "### Optional Enhancements:\n",
    "- Enable K-Fold cross-validation for more robust metrics\n",
    "- Try different augmentation factors\n",
    "- Experiment with hyperparameters\n",
    "- Add ensemble methods\n",
    "\n",
    "---\n",
    "\n",
    "**Good luck with your project! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
