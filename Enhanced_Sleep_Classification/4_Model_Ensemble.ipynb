{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32ee18ae",
   "metadata": {},
   "source": [
    "# Model Ensemble & Weighted Voting\n",
    "This notebook implements ensemble methods to combine multiple models for improved performance.\n",
    "\n",
    "## Why Ensemble Methods?\n",
    "- âœ… **Reduced Variance**: Different models make different errors\n",
    "- âœ… **Improved Accuracy**: Wisdom of crowds effect\n",
    "- âœ… **Better Generalization**: Less prone to overfitting\n",
    "- âœ… **Robustness**: More reliable predictions\n",
    "- âœ… **Confidence Estimation**: Agreement between models\n",
    "\n",
    "## Ensemble Techniques Implemented:\n",
    "1. **Simple Averaging**: Equal weight to all models\n",
    "2. **Weighted Averaging**: Weight by validation performance\n",
    "3. **Voting**: Majority vote for classification\n",
    "4. **Stacking**: Meta-learner on top of base models\n",
    "5. **K-Fold Ensemble**: Combine models from different folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f733e830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ee1ff5",
   "metadata": {},
   "source": [
    "## 1. Simple Averaging Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188ae10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_averaging_ensemble(models, X, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Ensemble using simple averaging of predictions\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    models : list\n",
    "        List of trained Keras models\n",
    "    X : array\n",
    "        Input data to predict on\n",
    "    threshold : float\n",
    "        Decision threshold for classification\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    y_pred_proba : array\n",
    "        Averaged prediction probabilities\n",
    "    y_pred : array\n",
    "        Binary predictions\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for model in models:\n",
    "        pred = model.predict(X, verbose=0)\n",
    "        predictions.append(pred.flatten())\n",
    "    \n",
    "    # Average predictions\n",
    "    y_pred_proba = np.mean(predictions, axis=0)\n",
    "    y_pred = (y_pred_proba > threshold).astype(int)\n",
    "    \n",
    "    return y_pred_proba, y_pred\n",
    "\n",
    "print(\"âœ… Simple averaging ensemble defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9d36f4",
   "metadata": {},
   "source": [
    "## 2. Weighted Averaging Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2b2334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_averaging_ensemble(models, X, weights=None, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Ensemble using weighted averaging of predictions\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    models : list\n",
    "        List of trained Keras models\n",
    "    X : array\n",
    "        Input data to predict on\n",
    "    weights : array or None\n",
    "        Weights for each model (should sum to 1)\n",
    "        If None, uses equal weights\n",
    "    threshold : float\n",
    "        Decision threshold for classification\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    y_pred_proba : array\n",
    "        Weighted averaged prediction probabilities\n",
    "    y_pred : array\n",
    "        Binary predictions\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for model in models:\n",
    "        pred = model.predict(X, verbose=0)\n",
    "        predictions.append(pred.flatten())\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    \n",
    "    # Use equal weights if not provided\n",
    "    if weights is None:\n",
    "        weights = np.ones(len(models)) / len(models)\n",
    "    else:\n",
    "        weights = np.array(weights)\n",
    "        weights = weights / weights.sum()  # Normalize\n",
    "    \n",
    "    # Weighted average\n",
    "    y_pred_proba = np.average(predictions, axis=0, weights=weights)\n",
    "    y_pred = (y_pred_proba > threshold).astype(int)\n",
    "    \n",
    "    return y_pred_proba, y_pred\n",
    "\n",
    "print(\"âœ… Weighted averaging ensemble defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52713f3e",
   "metadata": {},
   "source": [
    "## 3. Voting Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80733705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def voting_ensemble(models, X, threshold=0.5, voting_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Ensemble using majority voting\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    models : list\n",
    "        List of trained Keras models\n",
    "    X : array\n",
    "        Input data to predict on\n",
    "    threshold : float\n",
    "        Decision threshold for individual model predictions\n",
    "    voting_threshold : float\n",
    "        Fraction of models that must agree for positive prediction\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    y_pred : array\n",
    "        Binary predictions based on voting\n",
    "    vote_confidence : array\n",
    "        Fraction of models voting for positive class\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for model in models:\n",
    "        pred = model.predict(X, verbose=0)\n",
    "        pred_binary = (pred.flatten() > threshold).astype(int)\n",
    "        predictions.append(pred_binary)\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    \n",
    "    # Calculate voting confidence (fraction of models voting positive)\n",
    "    vote_confidence = np.mean(predictions, axis=0)\n",
    "    \n",
    "    # Final prediction based on voting threshold\n",
    "    y_pred = (vote_confidence >= voting_threshold).astype(int)\n",
    "    \n",
    "    return y_pred, vote_confidence\n",
    "\n",
    "print(\"âœ… Voting ensemble defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908d2140",
   "metadata": {},
   "source": [
    "## 4. Stacking Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa66afd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackingEnsemble:\n",
    "    \"\"\"\n",
    "    Stacking ensemble with meta-learner\n",
    "    \n",
    "    Uses base model predictions as features for a meta-learner\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_models, meta_learner=None):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        base_models : list\n",
    "            List of trained Keras models\n",
    "        meta_learner : sklearn classifier or None\n",
    "            Meta-learner model. If None, uses LogisticRegression\n",
    "        \"\"\"\n",
    "        self.base_models = base_models\n",
    "        self.meta_learner = meta_learner if meta_learner else LogisticRegression(max_iter=1000)\n",
    "    \n",
    "    def get_base_predictions(self, X):\n",
    "        \"\"\"\n",
    "        Get predictions from all base models\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for model in self.base_models:\n",
    "            pred = model.predict(X, verbose=0)\n",
    "            predictions.append(pred.flatten())\n",
    "        return np.column_stack(predictions)\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Train meta-learner on base model predictions\n",
    "        \"\"\"\n",
    "        # Get base model predictions\n",
    "        base_predictions = self.get_base_predictions(X_train)\n",
    "        \n",
    "        # Train meta-learner\n",
    "        self.meta_learner.fit(base_predictions, y_train)\n",
    "        print(f\"âœ… Meta-learner trained on {len(self.base_models)} base model predictions\")\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict probabilities using stacked ensemble\n",
    "        \"\"\"\n",
    "        base_predictions = self.get_base_predictions(X)\n",
    "        \n",
    "        if hasattr(self.meta_learner, 'predict_proba'):\n",
    "            return self.meta_learner.predict_proba(base_predictions)[:, 1]\n",
    "        else:\n",
    "            return self.meta_learner.predict(base_predictions)\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Predict classes using stacked ensemble\n",
    "        \"\"\"\n",
    "        proba = self.predict_proba(X)\n",
    "        return (proba > threshold).astype(int)\n",
    "\n",
    "print(\"âœ… Stacking ensemble defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc67632f",
   "metadata": {},
   "source": [
    "## 5. Calculate Optimal Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6589314d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_optimal_weights(models, X_val, y_val, metric='accuracy'):\n",
    "    \"\"\"\n",
    "    Calculate optimal weights for weighted ensemble based on validation performance\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    models : list\n",
    "        List of trained models\n",
    "    X_val, y_val : arrays\n",
    "        Validation data\n",
    "    metric : str\n",
    "        Metric to use for weighting ('accuracy', 'f1', 'roc_auc')\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    weights : array\n",
    "        Normalized weights for each model\n",
    "    scores : array\n",
    "        Individual model scores\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        y_pred_proba = model.predict(X_val, verbose=0).flatten()\n",
    "        y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "        \n",
    "        if metric == 'accuracy':\n",
    "            score = accuracy_score(y_val, y_pred)\n",
    "        elif metric == 'f1':\n",
    "            score = f1_score(y_val, y_pred)\n",
    "        elif metric == 'roc_auc':\n",
    "            score = roc_auc_score(y_val, y_pred_proba)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown metric: {metric}\")\n",
    "        \n",
    "        scores.append(score)\n",
    "        print(f\"Model {i+1} {metric}: {score:.4f}\")\n",
    "    \n",
    "    # Convert scores to weights (softmax-like)\n",
    "    scores = np.array(scores)\n",
    "    weights = scores / scores.sum()\n",
    "    \n",
    "    print(f\"\\nâœ… Calculated weights based on {metric}:\")\n",
    "    for i, (score, weight) in enumerate(zip(scores, weights)):\n",
    "        print(f\"   Model {i+1}: score={score:.4f}, weight={weight:.4f}\")\n",
    "    \n",
    "    return weights, scores\n",
    "\n",
    "print(\"âœ… Weight calculation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a90f2b0",
   "metadata": {},
   "source": [
    "## 6. Comprehensive Ensemble Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b97c4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ensemble_methods(models, X_test, y_test, weights=None):\n",
    "    \"\"\"\n",
    "    Compare different ensemble methods\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    models : list\n",
    "        List of trained models\n",
    "    X_test, y_test : arrays\n",
    "        Test data\n",
    "    weights : array or None\n",
    "        Weights for weighted averaging\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    results : dict\n",
    "        Dictionary with results for each ensemble method\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Evaluating Ensemble Methods on {len(models)} models\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # 1. Individual models\n",
    "    print(\"ðŸ“Š Individual Model Performance:\")\n",
    "    for i, model in enumerate(models):\n",
    "        y_pred_proba = model.predict(X_test, verbose=0).flatten()\n",
    "        y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "        \n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        \n",
    "        results[f'model_{i+1}'] = {\n",
    "            'accuracy': acc,\n",
    "            'f1_score': f1,\n",
    "            'roc_auc': auc,\n",
    "            'y_pred': y_pred,\n",
    "            'y_pred_proba': y_pred_proba\n",
    "        }\n",
    "        print(f\"   Model {i+1}: Acc={acc:.4f}, F1={f1:.4f}, AUC={auc:.4f}\")\n",
    "    \n",
    "    # 2. Simple averaging\n",
    "    print(\"\\nðŸ“Š Simple Averaging Ensemble:\")\n",
    "    y_pred_proba, y_pred = simple_averaging_ensemble(models, X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    results['simple_avg'] = {\n",
    "        'accuracy': acc,\n",
    "        'f1_score': f1,\n",
    "        'roc_auc': auc,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }\n",
    "    print(f\"   Acc={acc:.4f}, F1={f1:.4f}, AUC={auc:.4f}\")\n",
    "    \n",
    "    # 3. Weighted averaging\n",
    "    if weights is not None:\n",
    "        print(\"\\nðŸ“Š Weighted Averaging Ensemble:\")\n",
    "        y_pred_proba, y_pred = weighted_averaging_ensemble(models, X_test, weights)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        results['weighted_avg'] = {\n",
    "            'accuracy': acc,\n",
    "            'f1_score': f1,\n",
    "            'roc_auc': auc,\n",
    "            'y_pred': y_pred,\n",
    "            'y_pred_proba': y_pred_proba\n",
    "        }\n",
    "        print(f\"   Acc={acc:.4f}, F1={f1:.4f}, AUC={auc:.4f}\")\n",
    "    \n",
    "    # 4. Voting\n",
    "    print(\"\\nðŸ“Š Voting Ensemble:\")\n",
    "    y_pred, vote_conf = voting_ensemble(models, X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, vote_conf)\n",
    "    results['voting'] = {\n",
    "        'accuracy': acc,\n",
    "        'f1_score': f1,\n",
    "        'roc_auc': auc,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': vote_conf\n",
    "    }\n",
    "    print(f\"   Acc={acc:.4f}, F1={f1:.4f}, AUC={auc:.4f}\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\\n\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"âœ… Ensemble evaluation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3529d17b",
   "metadata": {},
   "source": [
    "## 7. Visualization: Ensemble Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c42580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ensemble_comparison(results, save_path=None):\n",
    "    \"\"\"\n",
    "    Visualize comparison of ensemble methods\n",
    "    \"\"\"\n",
    "    # Prepare data\n",
    "    methods = []\n",
    "    accuracies = []\n",
    "    f1_scores = []\n",
    "    aucs = []\n",
    "    \n",
    "    for method, metrics in results.items():\n",
    "        methods.append(method.replace('_', ' ').title())\n",
    "        accuracies.append(metrics['accuracy'])\n",
    "        f1_scores.append(metrics['f1_score'])\n",
    "        aucs.append(metrics['roc_auc'])\n",
    "    \n",
    "    # Create plot\n",
    "    x = np.arange(len(methods))\n",
    "    width = 0.25\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "    \n",
    "    bars1 = ax.bar(x - width, accuracies, width, label='Accuracy', alpha=0.8)\n",
    "    bars2 = ax.bar(x, f1_scores, width, label='F1-Score', alpha=0.8)\n",
    "    bars3 = ax.bar(x + width, aucs, width, label='ROC-AUC', alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Method', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Ensemble Methods Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(methods, rotation=45, ha='right')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.set_ylim([0, 1.05])\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels\n",
    "    def add_labels(bars):\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.3f}',\n",
    "                   ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    add_labels(bars1)\n",
    "    add_labels(bars2)\n",
    "    add_labels(bars3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"âœ… Comparison plot saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "print(\"âœ… Comparison plotting function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb4f494",
   "metadata": {},
   "source": [
    "## 8. ROC Curve Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8def9c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curves_comparison(results, y_test, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot ROC curves for all ensemble methods\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(results)))\n",
    "    \n",
    "    for (method, metrics), color in zip(results.items(), colors):\n",
    "        y_pred_proba = metrics['y_pred_proba']\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "        auc_score = metrics['roc_auc']\n",
    "        \n",
    "        plt.plot(fpr, tpr, color=color, lw=2, \n",
    "                label=f'{method.replace(\"_\", \" \").title()} (AUC = {auc_score:.3f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "    plt.title('ROC Curves - Ensemble Methods Comparison', fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc='lower right', fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"âœ… ROC curves saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "print(\"âœ… ROC curve plotting function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985531b4",
   "metadata": {},
   "source": [
    "## 9. Example Usage Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4475cdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (uncomment to use):\n",
    "# \n",
    "# # Assuming you have trained models from K-Fold CV\n",
    "# # models = [...]\n",
    "# # X_val, y_val = ...\n",
    "# # X_test, y_test = ...\n",
    "# \n",
    "# # Calculate optimal weights\n",
    "# weights, scores = calculate_optimal_weights(models, X_val, y_val, metric='roc_auc')\n",
    "# \n",
    "# # Evaluate all ensemble methods\n",
    "# results = evaluate_ensemble_methods(models, X_test, y_test, weights=weights)\n",
    "# \n",
    "# # Visualize results\n",
    "# plot_ensemble_comparison(results, save_path='ensemble_comparison.png')\n",
    "# plot_roc_curves_comparison(results, y_test, save_path='ensemble_roc_curves.png')\n",
    "# \n",
    "# # Use stacking ensemble\n",
    "# stacking = StackingEnsemble(models)\n",
    "# stacking.fit(X_val, y_val)\n",
    "# y_pred_stack = stacking.predict(X_test)\n",
    "# print(f\"Stacking accuracy: {accuracy_score(y_test, y_pred_stack):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… Model Ensemble utilities loaded successfully!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nAvailable functions:\")\n",
    "print(\"  - simple_averaging_ensemble(models, X, threshold)\")\n",
    "print(\"  - weighted_averaging_ensemble(models, X, weights, threshold)\")\n",
    "print(\"  - voting_ensemble(models, X, threshold, voting_threshold)\")\n",
    "print(\"  - StackingEnsemble(base_models, meta_learner)\")\n",
    "print(\"  - calculate_optimal_weights(models, X_val, y_val, metric)\")\n",
    "print(\"  - evaluate_ensemble_methods(models, X_test, y_test, weights)\")\n",
    "print(\"  - plot_ensemble_comparison(results, save_path)\")\n",
    "print(\"  - plot_roc_curves_comparison(results, y_test, save_path)\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
